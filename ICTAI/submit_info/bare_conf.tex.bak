
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, conference, compsocconf]{IEEEtran}
% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

%\usepackage{cite}
%\usepackage{amsmath,amssymb,amsfonts}
\usepackage{caption}
\usepackage[noend]{algpseudocode}
\usepackage{algorithmicx,algorithm}
\usepackage{graphicx}
%\usepackage{textcomp}
%\usepackage{xcolor}
%\usepackage{makecell}
\usepackage{multirow}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{On the convergence speed of AMSGRAD and beyond}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\author{\IEEEauthorblockN{Tao Tan, Shiqun Yin, Kunling Liu, Man Wan}
\IEEEauthorblockA{Faculty of Computer and Information Science\\
Southwest University\\
Chongqing, China 400715\\
1169311978@qq.com; Corresponding author: qqqq-qiong@163.com; 121398910@qq.com; 1812658139@qq.com}
%\and
%\IEEEauthorblockN{Shiqun Yin}
%\IEEEauthorblockA{Faculty of Computer and Information Science\\
%Southwest University\\
%Chongqing, China 400715\\
%Corresponding author: qqqq-qiong@163.com}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3},
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
In ICLR's (2018) best paper "On the Convergence of Adam and Beyond", the author points out the shortcomings in Adam's convergence proof, proposes an AMSGRAD algorithm that can guarantee convergence as the number of iterations increases. However, through some comparative experiments, this paper finds that there are two problems in the convergence process of AMSGRAD algorithm. Firstly, the AMSGRAD algorithm is easy to oscillate; Secondly, the AMSGRAD algorithm converges slowly. After analysis, the above two problems can be solved by the following ways. When $g_{t-1}g_t>0$, this paper adds the momentum term in Momentum algorithm to the AMSGRAD algorithm to accelerate convergence. When $g_{t-1}g_t\le0$, this paper use SGD algorithm instead of AMSGRAD algorithm to update the model weights. In order to eliminate some negative effects of the previous parameter gradient on the current parameter gradient and reduce the oscillation amplitude of the objective function, the first-order and second-order moment estimations of the parameter gradient are recalculated when $g_{t-1}g_t\le0$. Therefore, this paper proposes the ACADG algorithm, which not only can improve the convergence speed, suppress the oscillation amplitude of the objective function, but also can improve the accuracy of training and test data sets.

\end{abstract}

\begin{IEEEkeywords}
AMSGRAD algorithm; Momentum algorithm; oscillation amplitude; ACADG algorithm; convergence speed;

\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
At present, most learning algorithms in deep learning are based on iterative ideas, and the purpose is to find a set of network parameters, optimize the model weights, and minimize the objective function. The existing optimization algorithms are mainly based on the idea of SGD${^{[1-3]}}$ algorithm, and can be divided into two categories.

One is the momentum method, such as Momentum${^{[4]}}$ algorithm and NAG${^{[5]}}$ algorithm. Using the idea of physical momentum, these algorithms not only can suppress the oscillation amplitude of objective function, speed up the convergence of algorithm, but also can make the objective function jump out of the unsatisfactory local optimal solution, improve the accuracy of training and test data sets.

The other is the adaptive method${^{[6-7]}}$, which mainly includes RMSPROP${^{[8]}}$ algorithm, ADAGRAD${^{[9]}}$ algorithm, ADADELTA${^{[10]}}$ algorithm, ADAM${^{[11]}}$ algorithm and AMSGRAD${^{[12]}}$ algorithm. These algorithms can calculate the adaptive learning rate for each iteration, find the appropriate step sizes, speed up the model convergence, and reduce the loss value of training and test data sets.

The above algorithms have been recognized in many practical applications. For example, in image recognition, the idea of replacing SGD algorithm with ADAM algorithm can greatly improve the convergence speed, improve the accuracy of training and test data sets.

In "On the Convergence of Adam and Beyond", the author points out that the unbiased second-order moment estimation of the parameter gradient and the number of iteration steps do not always maintain a non-decreasing function relationship in the iterative process of the ADAM algorithm. Thus the adaptive learning rate and the number of iteration steps do not always maintain a non-increasing function relationship, and the uncertainty of the adaptive learning rate may lead to the phenomenon that the ADAM algorithm does not converge during the iterative process. In addition, they proposed the AMSGRAD algorithm. By recording the historical maximum of the unbiased second-order moment estimation of the parameter gradient, the non-decreasing of the unbiased second-order moment estimation of the parameter gradient and the non-increment of the adaptive learning rate are ensured, which ensures that the AMSGRAD algorithm always converges during the iterative process.

However, this paper finds two problems in AMSGRAD algorithm. Firstly, the AMSGRAD algorithm is easy to oscillate. Secondly, the AMSGRAD algorithm converges slowly. Therefore, this paper proposes the ACADG algorithm, which is a new adaptive learning rate optimization algorithm. In short, the main contributions of this paper are as follows:
\begin{enumerate}
  \item This paper finds that there are two problems in the convergence process of AMSGRAD algorithm, one is oscillation amplitude, the other is convergence speed.
  \item This paper proposes the ACADG algorithm and successfully solved the above two problems.
  \item Through synthetic experiment, logistic regression and DNN experiment, CNN experiment, it is verified that ACADG algorithm not only has faster convergence speed, lower oscillation amplitude, but also performs better in training and test data sets than AMSGRAD and ADAM algorithms.
\end{enumerate}
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)


\section{Related work}
This section is divided into two parts. The first part introduces the Momentum algorithm, which mainly includes the update process of model weights in different iteration stages. The second part introduces the AMSGRAD algorithm, and compares the parameter update process of the AMSGRAD algorithm with that of the ADAM algorithm.
\subsection{Momentum algorithm}
In physics, people use momentum to simulate the inertia of an object as it moves. Through derivation, it is found that the momentum idea can be used in the deep learning optimization algorithm to improve the convergence speed. Then the Momentum algorithm is proposed, and the update rules of model weights are designed as follows:

\centerline{$\bigtriangleup w_t=\rho\bigtriangleup w_{t-1}-\eta g_t$}

\centerline{$w_t=w_{t-1}+\bigtriangleup w_t$}

Where $w$ is the model weights, $t$ is the number of iteration steps, $\rho$ is the momentum factor, $\eta$ is the learning rate, $g$ is the parameter gradient.

After analysis, the iterative process of the Momentum algorithm can be divided into three stages.

In the initial stage of the iteration, the angle between the parameter gradient vector in the previous step and the parameter gradient vector in the current step is less than 90 degrees, that is $g_{t-1}g_t>0$, the momentum term can help the algorithm to accelerate convergence.

In the middle stage of the iteration, the momentum factor can increase the update range of the model weights, so that the objective function can jump out of the unsatisfactory local optimal solution as much as possible.

In the later stage of the iteration, the angle between the parameter gradient vector in the previous step and the parameter gradient vector in the current step is greater than 90 degrees, that is $g_{t-1}g_t\le0$. The objective function will oscillate around the relatively satisfactory local optimal value. The momentum factor will reduce the update range of the model weights, suppress the amplitude of the objective function, and make the objective function converge to the relatively satisfactory local optimal solution more quickly.

To sum up, the momentum term not only can improve the convergence speed of the algorithm, but also can appropriately suppress the amplitude of the objective function. It can also help the objective function to jump out of the unsatisfactory local optimal solution, improve the accuracy of training and test data sets.

\subsection{AMSGRAD algorithm}
AMSGRAD algorithm is a new exponential moving average gradient optimization algorithm, and its purpose is to solve the convergence problem of ADAM algorithm. The specific implementation steps of AMSGRAD algorithm are as follows:
\begin{enumerate}
  \item Calculate the biased first-order ($m$) and second-order ($v$) moment estimators of parameter gradient, respectively.
  \item Use $m$ and $v$ to calculate the unbiased first-order ($\widehat{m}$) and second-order ($\widehat{v}$) moment estimators of parameter gradient, respectively.
  \item Record the historical maximum ($v_{max}$) of unbiased second-order moment estimator of parameter gradient.
  \item Update the model weights with following formula:

  \centerline{$w_t=w_{t-1}-\frac{\eta}{\sqrt{v_{max}+\epsilon}}\widehat{m}_t$}

  \item Repeat the above four steps until meet the stopping criterion.
\end{enumerate}

Where $w$ is the model weights, $t$ is the number of iteration steps, $\eta$ is the learning rate, $\epsilon$ is a constant and used to prevent the denominator from being 0.

The biggest difference between AMSGRAD algorithm and ADAM algorithm in the update process of model weights is the calculation method of unbiased second-order moment estimation of parameter gradient. The ADAM algorithm directly uses the unbiased second-order moment estimation of the parameter gradient to calculate the adaptive learning rate, while the AMSGRAD algorithm uses the historical maximum of the unbiased second-order moment estimation of the parameter gradient to calculate the adaptive learning rate. Thus, it is found that the AMSGRAD algorithm ensures the convergence of the model by making the adaptive learning rate maintain a non-incremental functional relationship with the iterative steps.


\section{ACADG Algorithm}
This section is divided into three parts. The first part introduces two disadvantages of the AMSGRAD algorithm in the iterative process. The second part introduces the ACADG algorithm. The third part introduces the performance of ACADG algorithm in convergence speed and oscillation amplitude.

\subsection{Disadvantages of the AMSGRAD algorithm}
Through the derivation of mathematical formulas${^{[12]}}$, the AMSGRAD algorithm proves that it can overcome the non-convergence problem in ADAM algorithm, and guarantees that the optimization algorithm converges with the increase of the number of iterations. However, this paper finds the following two problems from the convergence process of the AMSGRAD algorithm:
\begin{enumerate}
  \item In order to ensure the convergence of the AMSGRAD algorithm, the AMSGRAD algorithm must record the historical maximum of the unbiased second-order moment estimate of the parameter gradient, and use it to calculate the adaptive learning rate. However, the adaptive learning rate of the AMSGRAD algorithm is lower than that of the ADAM algorithm during the iterative process, which will increase convergence time, reduce convergence speed.

  \item When $g_{t-1}g_t\le0$, the angle between the parameter gradient vector in the previous step and the parameter gradient vector in the current step is greater than 90 degrees. Using the AMSGRAD algorithm to update the model weights will cause the unbiased first-order moment estimate of the parameter gradient in the previous step to have a negative effect on the current parameter gradient vector. Sometimes, this negative effect will last for a long time, and even cause the objective function to experience severe oscillations during convergence.
\end{enumerate}

\subsection{The ACADG algorithm}
In order to solve the above two problems of AMSGRAD algorithm, this paper designs ACADG algorithm, as shown below:

\begin{algorithm}[H]
\caption{ACADG} %算法的名字
\hspace*{0.02in} {\bf Input:} %算法的输入， \hspace*{0.02in}用来控制位置，同时利用 \\ 进行换行
$\eta_0$, $\beta_1$, $\beta_2$, $m_0$, $v_0$, $\bigtriangleup w_{0}$, $T$, $\epsilon$, $\varepsilon$\\
\hspace*{0.02in} {\bf Output:} %算法的结果输出
$w$
\begin{algorithmic}[1]
\For{${t=1,2,3,\cdots,T}$} % For 语句，需要和EndFor对应
    \State $\eta_t=\frac{\eta_0}{\sqrt{t}}$
    \State $g_t=\bigtriangledown f(w_{t-1})$
    \State $m_t=\beta_1m_{t-1}+(1-\beta_1)g_t$
    \State $v_t=\beta_2v_{t-1}+(1-\beta_2)g^2_t$
    \State $\widehat{m}_t=\frac{m_t}{1-\beta^t_1}$
    \State $\widehat{v}_t=\frac{v_t}{1-\beta^t_2}$
    \State $\widehat{v}_t=max(\widehat{v}_{t-1},\widehat{v}_t)$
    \If{$g_{t-1}g_t\le0$}
        \State $w_t=w_{t-1}-\eta_tg_t$
        \State $m_t=g_t$
        \State $v_t=g^2_t$
    \Else
        \State $w_t=w_{t-1}-\frac{\eta_t}{\sqrt{v_{max}+\epsilon}}\widehat{m}_t+\beta_1\bigtriangleup w_{t-1}$
    \EndIf
    \State $\bigtriangleup w_{t}=w_t-w_{t-1}$
    \If{$f(w_t)\leq \varepsilon$} % If 语句，需要和EndIf对应
　　　 \State \Return $w_t$
    \EndIf
\EndFor
\State \Return $w_t$
\end{algorithmic}
\end{algorithm}

where $\eta_0$ is the initial learning rate, $\beta_1$ is the coefficient of the first-order moment estimation of parameter gradient, $\beta_2$ is the coefficient of the second-order moment estimation of parameter gradient, $m_0$ is the initial first-order moment estimation of parameter gradient, $v_0$ is the initial second-order moment estimation of parameter gradient, $\bigtriangleup w_{0}$ is the initial value of the model weights differences, $T$ is the number of iterations, $\epsilon$ is a constant and used to prevent the denominator from being 0, $\varepsilon$ is the minimum loss value that satisfies the stopping condition, $w$ is the model weights.


\subsection{The performance of ACADG algorithm}
Compared with the AMSGRAD algorithm, it is found that the ACADG algorithm has the following three changes.
\begin{enumerate}
  \item The ACADG algorithm uses an exponential decay learning rate method in model training.
  \item When $g_{t-1}g_t>0$, the ACADG algorithm adds the momentum term to the AMSGRAD algorithm based on the idea of the Momentum algorithm.
  \item When $g_{t-1}g_t\le0$, the ACADG algorithm uses the SGD algorithm to update the model weights, and recalculates the first-order and second-order moment estimates of the parameter gradient.
\end{enumerate}

The ACADG algorithm divides the training process into two cases by computing $g_{t-1}g_t$.

When $g_{t-1}g_t>0$, the momentum term has a positive effect on the first-order moment estimation of the parameter gradient in the current steps. Through the momentum term, the ACADG algorithm can speed up the decline of the loss function, and make the loss function quickly reach a satisfactory local optimal solution. Therefore, the ACADG algorithm accelerates the convergence speed of the model.

When $g_{t-1}g_t\le0$, whether the AMSGRAD algorithm or the ADAM algorithm, the first-order moment estimation of the parameter gradient in the previous steps will have a negative effect on the parameter gradient vector in the current steps. Sometimes, this negative effect will last for a long time, and even cause the objective function to experience severe oscillations during convergence. In this case, the ACADG algorithm uses the SGD algorithm to update the model weights. Regardless of the negative effect, the ACADG algorithm will not be affected. Therefore, the ACADG algorithm reduces the oscillation amplitude of the loss function.

In addition, by recalculating the first-order and second-order moment estimates of the parameter gradient and using the exponential decay learning rate, the ACADG algorithm can quickly jump out of the unsatisfactory local optimal solution, improve the accuracy of training and test data sets, and also can help the model to quickly converge to satisfactory local optimal solution and reduce the oscillation  amplitude of the loss function.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.



\section{Experiments}
In order to verify the effectiveness of the ACADG algorithm, this paper designs three experiments. The first one is a synthetic experiment, which uses two different objective functions to verify the convergence, convergence speed and oscillation amplitude of ACADG algorithm. The next one is a logistic regression and DNN experiment, which uses different models to verify the convergence speed and the oscillation amplitude of ACADG algorithm on convex optimization and non-convex optimization problems. The last one is a CNN experiment, which uses two different data sets to verify the versatility of the ACADG algorithm. In the three experiments, the ADAM and AMSGRAD algorithms are used as comparison objects. The convergence of the algorithm, the convergence speed of the model, the oscillation amplitude of the objective function, the accuracy on training and test data sets are used as the evaluation criteria for measuring the performance of the three algorithms. The specific code for all experiments is at $https://github.com/Tantao122200/acadg\_new$.

\begin{figure}[!t]
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//syn//online.png}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//syn//stochastic.png}
\end{minipage}%
\centering
\captionsetup{font={scriptsize}}
\caption {The performance of the three algorithms on the synthetic data. The first and second images represent the results of the three algorithms in the first and second objective functions, respectively.}
\end{figure}

\subsection{Synthetic experiment}
In order to verify the convergence, convergence speed and oscillation amplitude of ACADG algorithm in synthetic experiment. This paper designs two different objective functions${^{[12]}}$, and as follows:

\begin{equation}
f_t(x)=\left\{
\begin{array}{rcl}
1010x & & {t\%101=1}\\
-10x & & {otherwise}\\
\end{array} \right.
\end{equation}

\begin{equation}
f_t(x)=\left\{
\begin{array}{rcl}
1010x & & {p=0.01}\\
-10x & & {otherwise}\\
\end{array} \right.
\end{equation}

Where $x$ belongs to a constraint set and $x\in[-1,1]$, $t$ is the number of iteration steps, $p$ is the probability and $p\in[0,1]$.

As can be seen from the above settings, whether the first objective function or the second objective function, the minimum value is obtained at $x = -1$.

In the ACADG algorithm, the relevant experimental parameters $\eta_0$, $\beta_1$, $\beta_2$, $m_0$, $v_0$, $\bigtriangleup w_{0}$, $T$, $\epsilon$, $\varepsilon$ are set as 0.01, 0.9, 0.999, 0, 0, 0, 1e7, 1e-8, 1e-9, respectively. The experimental parameters related to the AMSGRAD and ADAM algorithms are consistent with those in ACADG algorithm, and the experimental results are shown as Flg.1.

As can be seen from Fig.1, the three algorithms have the following three phenomena in the first objective function and the second objective function:
\begin{enumerate}
  \item As the number of iterations increases, the ADAM algorithm converges to 1, but both the AMSGRAM algorithm and ACADG algorithm converge to -1.
  \item The convergence speed of the objective function in the ACADG algorithm is significantly faster than that of the AMSGRAD algorithm.
  \item During the convergence process, the oscillation amplitude of the AMSGRAD algorithm is much larger than that of the ACADG algorithm.

\end{enumerate}
\begin{figure}[!t]
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//logic//mnist_train_acc.png}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//logic//mnist_train_loss.png}
\end{minipage}%
\centering
\captionsetup{font={scriptsize}}
\caption {The training performance of the three algorithms on logistic regression model. The first and second images represent the accuracy and loss on Mnist training data set, respectively.}
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//logic//mnist_test_acc.png}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//logic//mnist_test_loss.png}
\end{minipage}%
\centering
\captionsetup{font={scriptsize}}
\caption {The test performance of the three algorithms on logistic regression model. The first and second images represent the accuracy and loss on Mnist test data set, respectively.}
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//DNN//mnist_train_acc.png}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//DNN//mnist_train_loss.png}
\end{minipage}%
\centering
\captionsetup{font={scriptsize}}
\caption {The training performance of the three algorithms on DNN model. The first and second images represent the accuracy and loss on Mnist training data set, respectively.}
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//DNN//mnist_test_acc.png}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//DNN//mnist_test_loss.png}
\end{minipage}%
\centering
\captionsetup{font={scriptsize}}
\caption {The test performance of the three algorithms on DNN model. The first and second images represent the accuracy and loss on Mnist test data set, respectively.}
\end{figure}

\subsection{Logistic regression and DNN experiment}
In order to verify the performance of ACADG algorithm on convex optimization problem and non-convex optimization problem. This paper uses the Mnist data set to perform experiments on logistic regression model and DNN model, respectively.

Mnist is a handwritten digital recognition image set, and which is commonly used in deep learning. It contains 60,000 black and white images with 28*28, and 50,000 samples in training data set, 10,000 samples in test data set, 10 classification labels.

Logistic regression model has only the input layer of 784 and the output layer of 10. DNN model has a hidden layer in addition to the input layer and the output layer in Logistic regression model, where the number of nodes is 100. In addition, the cross entropy loss function is used as the objective function in the experiment.

In the ACADG algorithm, the relevant experimental parameters $\eta_0$, $\beta_1$, $\beta_2$, $m_0$, $v_0$, $\bigtriangleup w_{0}$, $T$, $\epsilon$, $\varepsilon$ are set as 0.001, 0.9, 0.999, 0, 0, 0, 1e5, 1e-8, 1e-9, respectively. The experimental parameters related to the AMSGRAD and ADAM algorithms are consistent with those in ACADG algorithm, and the experimental results are shown as Flg.2-5 and Table1.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{The performance of the three algorithms on logistic regression and DNN model.}\label{tab1}
\centering
%\captionsetup{font={scriptsize}}
%\caption{The performance of the three algorithms on logistic regression model and DNN model.}
\begin{tabular}{|c|c|c|c|c|}
    \hline
    \multirow{2}*{Algorithm} &
    \multicolumn{2}{c|}{Logistic regression} &
    \multicolumn{2}{c|}{DNN}\\
    \cline{2-5}
    & Train & Test & Train & Test\\ \hline
    ADAM &  88.08\% & 88.83\% & 92.62\% & 92.65\%\\ \hline
    AMSGRAD &  87.05\% & 88.12\% & 92.34\% & 92.49\%\\ \hline
    ACADG &  92.16\% & 92.28\% & 97.77\% & 96.91\%\\ \hline
\end{tabular}
\end{table}

From the training loss of Fig.2, the ACADG algorithm reaches the convergence point of the model faster than the AMSGRAD and ADAM algorithms under the same number of iterations. For example, when the number of iteration steps is 20,000, the training loss of the ADAM and AMSGRAD algorithms is still in a decreasing state, but the ACADG algorithm has reached the convergence point and oscillated between the local optimal solution. From the test loss in Fig.3, the loss value of the AMSGRAD algorithm on the test data set is higher than that of the ADAM algorithm, which indicates that the lower learning rate in the AMSGRAD algorithm reduces the performance of the model. From Fig.4 and Fig.5, the ACADG algorithm has higher accuracy and lower loss than the AMSGRAD and ADAM algorithms in the training and test data sets.

As can be seen from Table1, the performance of Mnist data set on logistic regression model is as follows: the accuracy of ACADG algorithm on the training data set is 5.11\% and 4.08\% higher than that of AMSGRAD and ADAM algorithms, respectively; the accuracy of ACADG algorithm on the test data set is 4.14\% and 3.45\% higher than that of AMSGRAD and ADAM algorithms, respectively. The performance of Mnist data set on DNN model is as follows: the accuracy of ACADG algorithm on the training data set is 5.43\% and 5.15\% higher than that of AMSGRAD and ADAM algorithms, respectively; the accuracy of ACADG algorithm on the test data set is 4.42\% and 4.26\% higher than that of AMSGRAD and ADAM algorithms, respectively.

Therefore, whether it is a convex optimization problem or a non-convex optimization problem, the ACADG algorithm is superior to AMSGRAD and ADAM algorithms in terms of the convergence speed, the oscillation amplitude, the accuracy of training and test data sets.

\begin{figure}[!t]
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//CNN//mnist//mnist_train_acc.png}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//CNN//mnist//mnist_train_loss.png}
\end{minipage}%
\centering
\captionsetup{font={scriptsize}}
\caption {The training performance of the three algorithms on CNN model. The first and second images represent the accuracy and loss on Mnist training data set, respectively.}
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//CNN//mnist//mnist_test_acc.png}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//CNN//mnist//mnist_test_loss.png}
\end{minipage}%
\centering
\captionsetup{font={scriptsize}}
\caption {The test performance of the three algorithms on CNN model. The first and second images represent the accuracy and loss on Mnist test data set, respectively.}
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//CNN//cifar-10//cifar_train_acc.png}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//CNN//cifar-10//cifar_train_loss.png}
\end{minipage}%
\centering
\captionsetup{font={scriptsize}}
\caption {The training performance of the three algorithms on CNN model. The first and second images represent the accuracy and loss on Cifar-10 training data set, respectively.}
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//CNN//cifar-10//cifar_test_acc.png}
\end{minipage}%
\begin{minipage}[t]{.5\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{picture//CNN//cifar-10//cifar_test_loss.png}
\end{minipage}%
\centering
\captionsetup{font={scriptsize}}
\caption {The test performance of the three algorithms on CNN model. The first 、、、、and second images represent the accuracy and loss on Cifar-10 test data set, respectively.}
\end{figure}

\subsection{CNN experiment}
In order to verify the performance of ACADG algorithm on complex neural networks. This paper performs experiment on CNN model using the Mnist and Cifar-10 data sets, respectively.

Cifar-10 is a common data set for deep learning, which consists of 60,000 RGB color images of 32*32, and covering 10 categories: airplanes, cars, birds, furs, deer, dogs, frogs, horses, boats, trucks. The training data set includes 50,000 samples, and the test data set includes 10,000 samples.

In the CNN model, this paper uses the dropout mechanism. Dropout means that some working nodes in the network will become inactive nodes with a certain probability during the model training. These inactive nodes can be temporarily not considered part of the network structure, but their weights must be preserved because they may become working nodes in the next iteration. In addition, the cross entropy loss function is used as the objective function in the experiment.

On the Mnist data set, the CNN model is composed of input[-1, 28, 28, 1], convolution[5, 5, 1, 32], pooling[2*2], convolution[5, 5, 32, 64], pooling[2*2], flat[7*7*64], hidden layer[1024], output[10]. The convolution operation has a step size[1,1,1,1], the convolution kernel[5,5], and the padding "SAME"; the pooling operation uses max\_pool form, and the pooling step size[1,2,2,1], the padding "SAME".

On the Cifar-10 data set, the CNN model is composed of input[-1, 32, 32, 3], convolution[5, 5, 3, 16], pooling[2*2], convolution[5, 5, 16, 32], pooling[2*2], flat[8*8*32], hidden layer[100], dropout(0.5), output[10]. The convolution operation has a step size[1,1,1,1], the convolution kernel[5,5], and the padding "SAME"; the pooling operation uses max\_pool form, and the pooling step size[1,2,2,1], the padding "SAME".

In the ACADG algorithm, the relevant experimental parameters $\eta_0$, $\beta_1$, $\beta_2$, $m_0$, $v_0$, $\bigtriangleup w_{0}$, $T_{Mnist}(T_{Cifar})$, $\epsilon$, $\varepsilon$ are set as 0.0001, 0.9, 0.999, 0, 0, 0, 1e5(5e4), 1e-8, 1e-9, respectively. The experimental parameters related to AMSGRAD and ADAM algorithms are consistent with those in ACADG algorithm, and the experimental results are shown as Flg.6-9 and Table2.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\centering
\caption{The performance of the three algorithms on CNN model.}\label{tab2}
\begin{tabular}{|c|c|c|c|c|}
    \hline
    \multirow{2}*{algorithm} &
    \multicolumn{2}{c|}{Mnist data} &
    \multicolumn{2}{c|}{Cifar-10 data}\\
    \cline{2-5}
    & Train & Test & Train & Test\\ \hline
    ADAM &  96.09\% & 96.41\% & 61.72\% & 61.06\%\\ \hline
    AMSGRAD &  96.88\% & 96.10\% & 49.22\% & 47.46\%\\ \hline
    ACADG &  99.22\% & 99.22\% & 65.63\% & 63.05\%\\ \hline
\end{tabular}
\end{table}

From Fig.6 and Fig.7, the ACADG algorithm has higher accuracy than the AMSGRAD and ADAM algorithms in the training and test data sets. From Fig.8 and Fig.9, the lower adaptive learning rate in the AMSGRAD algorithm seriously affects the accuracy of the model, but the ACADG algorithm is not affected by the historical maximum of the unbiased second-order moment estimation of the parameter gradient, and the performance of the ACADG algorithm is even better than that of the ADAM algorithm.

As can be seen from Table2, the performance of Mnist data set on CNN model is as follows: the accuracy of ACADG algorithm on the training data set is 2.44\% and 3.13\% higher than that of AMSGRAD and ADAM algorithms, respectively; the accuracy of ACADG algorithm on the test data set is 3.10\% and 2.81\% higher than that of AMSGRAD and ADAM algorithms, respectively. The performance of Cifar-10 data set on CNN model is as follows: the accuracy of ACADG algorithm on the training data set is 16.41\% and 3.91\% higher than that of AMSGRAD and ADAM algorithms, respectively; the accuracy of ACADG algorithm on the training data set is 15.59\% and 1.99\% higher than that of AMSGRAD and ADAM algorithms, respectively.

It can be seen from the above three experiments: in the experiments of convex optimization problem, non-convex optimization problem, complex model, different data sets, ACADG algorithm is superior to AMSGRAD and ADAM algorithms in the convergence speed, the oscillation amplitude, the accuracy of training and test data sets. In fact, after many experiments, this paper finds that ACADG algorithm is more robust to the model weights than ADAM and AMSGRAD algorithms.

\section{Conclusion}
This paper proposes ACADG algorithm, which is an adaptive gradient optimization algorithm. The ACADG algorithm not only can improve the convergence speed, suppress the oscillation amplitude of objective function, but also can improve the accuracy of training and test data sets. Through some comparative experiments, it is found that ACADG algorithm is superior to AMSGRAD and ADAM algorithms in above three aspects, whether it is convex optimization problems or non-convex optimization problems.


\section*{Acknowledgment}
This work is supported by the Science \& Technology project (41008114, 41011215, and 41014117). Corresponding author: Shiqun Yin, qqqq-qiong@163.com.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\begin{thebibliography}{1}
\bibitem{[1]} Zhang, Sixin, A. Choromanska, and Y. Lecun. "Deep learning with Elastic Averaging SGD." (2014):685-693.
\bibitem{[2]} Chakroun, Imen, T. Haber, and T. J. Ashby. "SW-SGD: The Sliding Window Stochastic Gradient Descent Algorithm." (2017):2318-2322.
\bibitem{[3]} Zhang, Wei, et al. "Staleness-aware async-SGD for distributed deep learning." (2016):2350-2356.
\bibitem{[4]} Qian, Ning. "On the momentum term in gradient descent learning algorithms." (1999):145-151.
\bibitem{[5]} Mukherjee, Indraneel, et al. "Parallel Boosting with Momentum." (2013):17-32.
\bibitem{[6]} Leimkuhler, Benedict, and X. Shang. "Adaptive Thermostats for Noisy Gradient Systems." (2015).
\bibitem{[7]} Arablouei, Reza, S. Werner, and K. Dogancay. "Adaptive frequency estimation of three-phase power systems with noisy measurements." (2013):2848-2852.
\bibitem{[8]} Kurbiel, Thomas, and S. Khaleghian. "Training of Deep Neural Networks based on Distance Measures using RMSProp." (2017).
\bibitem{[9]} Mukkamala, Mahesh Chandra, and M. Hein. "Variants of RMSProp and Adagrad with Logarithmic Regret Bounds." (2017).
\bibitem{[10]} Zeiler, Matthew D. "ADADELTA: An Adaptive Learning Rate Method." (2012).
\bibitem{[11]} Kingma, Diederik, and J. Ba. "Adam: A Method for Stochastic Optimization." (2014).
\bibitem{[12]} Sashank J. Reddi, Satyen Kale and Sanjiv Kumar. "On The Convergence of ADAM and Beyond." (2018).
\end{thebibliography}




% that's all folks
\end{document}


